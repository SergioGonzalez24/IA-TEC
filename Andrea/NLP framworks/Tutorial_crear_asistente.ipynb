{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVMOE4C-3p37",
        "outputId": "b88b78d4-28bd-41fc-b42e-3dcb9d494519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.2-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "FNKpqbvJ4CbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = \"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\"\n",
        "client = OpenAI(api_key=api_key)"
      ],
      "metadata": {
        "id": "xXUP2EWx4NZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How Assistants work Beta\n",
        "The Assistants API is designed to help developers build powerful AI assistants capable of performing a variety of tasks.\n",
        "\n",
        "\n",
        "Assistants can access persistent Threads. Threads simplify AI application development by storing message history and truncating it when the conversation gets too long for the model’s context length. You create a Thread once, and simply append Messages to it as your users reply.\n",
        "Assistants can access Files in several formats — either as part of their creation or as part of Threads between Assistants and users. When using tools, Assistants can also create files (e.g., images, spreadsheets, etc) and cite files they reference in the Messages they create."
      ],
      "metadata": {
        "id": "jr3xMrQtfvET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objects\n",
        "\n",
        "\n",
        "OBJECT\tWHAT IT REPRESENTS\n",
        "* **Assistant**:\tPurpose-built AI that uses OpenAI’s models and calls tools\n",
        "\n",
        "* **Thread**: \tA conversation session between an Assistant and a user. Threads store Messages and automatically handle truncation to fit content into a model’s context.\n",
        "\n",
        "* **Message**: \tA message created by an Assistant or a user. Messages can include text, images, and other files. Messages stored as a list on the Thread.\n",
        "\n",
        "* **Run**:\tAn invocation of an Assistant on a Thread. The Assistant uses it’s configuration and the Thread’s Messages to perform tasks by calling models and tools. As part of a Run, the Assistant appends Messages to the Thread.\n",
        "\n",
        "* **Run Step**:\tA detailed list of steps the Assistant took as part of a Run. An Assistant can call tools or create Messages during it’s run. Examining Run Steps allows you to introspect how the Assistant is getting to it’s final results."
      ],
      "metadata": {
        "id": "nxh0jBuKgIV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 1.** Crear un asistente"
      ],
      "metadata": {
        "id": "GGnpzIb_yV_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get started, creating an Assistant only requires specifying the model to use. But you can further customize the behavior of the Assistant:\n",
        "\n",
        "1. Use the instructions parameter to guide the personality of the Assistant and define it’s goals. Instructions are similar to system messages in the Chat Completions API.\n",
        "\n",
        "2. Use the tools parameter to give the Assistant access to up to 128 tools. You can give it access to OpenAI-hosted tools like code_interpreter and retrieval, or call a third-party tools via a function calling.\n",
        "\n",
        "3. Use the file_ids parameter to give the tools like code_interpreter and retrieval access to files. Files are uploaded using the File upload endpoint and must have the purpose set to assistants to be used with this API."
      ],
      "metadata": {
        "id": "xum79gsAgxRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name = \"Math tutor\",\n",
        "    instructions = \"You are a personal math tutor. Write and run to answer the math questions\",\n",
        "    tools = [{\"type\":\"code_interpreter\"}],\n",
        "    model='gpt-4-1106-preview'\n",
        ")"
      ],
      "metadata": {
        "id": "dcDBegWy6JbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "WIpOuf-7gB2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 2.** Crear un Thread"
      ],
      "metadata": {
        "id": "17gMhcLk_VKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ],
      "metadata": {
        "id": "W8419Mv-Es1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1981b9-be1a-4693-d4b5-83c142b591ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread(id='thread_OZDl9CCaP8DDD9V744q4zxIY', created_at=1700186967, metadata={}, object='thread')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 3.** Agregar un mensaje al thread."
      ],
      "metadata": {
        "id": "6BwemeRf_6NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Solve this problem: 3x + 23 = 70\"\n",
        ")\n",
        "\n",
        "print(message)"
      ],
      "metadata": {
        "id": "qgyRbb4uKcdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7845ced-2b9a-40d7-cdc4-6bad175ea589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThreadMessage(id='msg_1BuYf7L5rNgYhEA6vgWGPqYy', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Solve this problem: 3x + 23 = 70'), type='text')], created_at=1700186977, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_OZDl9CCaP8DDD9V744q4zxIY')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 4.** Ejecutar el asistente"
      ],
      "metadata": {
        "id": "2rQSf-GXAmeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id = thread.id,\n",
        "  assistant_id = assistant.id,\n",
        ")"
      ],
      "metadata": {
        "id": "Ww_MNQidKvdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 5.** Desplegar respuestas del asistente"
      ],
      "metadata": {
        "id": "_dC-CJXDA7tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "  thread_id = thread.id,\n",
        "  run_id = run.id\n",
        ")"
      ],
      "metadata": {
        "id": "vyMyuKK9LF2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KC-dICO7N8k4",
        "outputId": "ccaf2b98-ca84-4fd7-9eb1-cba83a76bf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in_progress'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id = thread.id\n",
        ")"
      ],
      "metadata": {
        "id": "VBhZBbFSLbJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d53DqgKyBx0R",
        "outputId": "e948b469-30f7-40b1-84b4-6e2e9d895b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ThreadMessage(id='msg_IlVxSQr6HtxDXK5SxMvLSqSf', assistant_id='asst_RdkP0i3Mivvokq1S3PSI1jI2', content=[MessageContentText(text=Text(annotations=[], value='The solution to the equation \\\\(3x + 23 = 70\\\\) is \\\\( x = \\\\frac{47}{3} \\\\) or approximately \\\\(15.67\\\\) when expressed as a decimal.'), type='text')], created_at=1700186999, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_Dc8WCJGVxWLWFHrttrgeDReX', thread_id='thread_OZDl9CCaP8DDD9V744q4zxIY')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for message in reversed(messages.data):\n",
        "    print(message.role + \": \" + message.content[0].text.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMoqlyoWL2Mm",
        "outputId": "bbe86334-7eda-49a3-baa4-5731ed791fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: Solve this problem: 3x + 23 = 70\n",
            "assistant: The solution to the equation \\(3x + 23 = 70\\) is \\( x = \\frac{47}{3} \\) or approximately \\(15.67\\) when expressed as a decimal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI assistant with Files"
      ],
      "metadata": {
        "id": "aVrIUbZITz81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 1.** Agregar un archivo"
      ],
      "metadata": {
        "id": "baYSpARHX4JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = client.files.create(\n",
        "    file=open(\"gan.pdf\",\"rb\"),\n",
        "    purpose = \"assistants\"\n",
        ")"
      ],
      "metadata": {
        "id": "ycG1bsunTtE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRKQeRv2VMaA",
        "outputId": "344c53d6-6c94-4668-a898-a6a6e3b5d2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FileObject(id='file-dM57xjXuRo1CxvjMEvhR7mXQ', bytes=530482, created_at=1700175086, filename='gan.pdf', object='file', purpose='assistants', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 2.** Crear un asistente"
      ],
      "metadata": {
        "id": "eVSwV2QHX-nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name = \"Machine learning researcher\",\n",
        "    instructions = \"You are a machine learning researcher. Answer questions on the research paper\",\n",
        "    tools = [{\"type\":\"retrieval\"}],\n",
        "    model='gpt-4-1106-preview',\n",
        "    file_ids = [\"file-dM57xjXuRo1CxvjMEvhR7mXQ\"]\n",
        ")"
      ],
      "metadata": {
        "id": "RC4bUpLPVTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 3.** Crear un Thread"
      ],
      "metadata": {
        "id": "swgo08E9YClT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uut4YvbJWC4T",
        "outputId": "2125f721-e296-4103-9475-03d29d0f45fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread(id='thread_F7vsUB3KowmbCegXNLGQMsDD', created_at=1700175307, metadata={}, object='thread')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 4.** Agregar un mensaje al thread."
      ],
      "metadata": {
        "id": "Fldquy9VYHk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"Can you explain me how works the generator loss function?\"\n",
        ")\n",
        "\n",
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OFPQjnWGFX",
        "outputId": "fc4ea7a1-7f94-4d22-9df0-179b897447bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThreadMessage(id='msg_H38zm1wy0T0AW7eGDGA2f5jr', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Can you explain me how works the generator loss function?'), type='text')], created_at=1700175377, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_F7vsUB3KowmbCegXNLGQMsDD')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 5.** Ejecutar el asistente"
      ],
      "metadata": {
        "id": "5MvHQG56YUip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id = thread.id,\n",
        "  assistant_id = assistant.id,\n",
        ")"
      ],
      "metadata": {
        "id": "o7wJKVTAWXEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run.status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3PWjKb9xWjSR",
        "outputId": "d8b825f6-3908-4321-e422-ab45110e513d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'queued'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Paso 6.** Desplegar respuestas del asistente"
      ],
      "metadata": {
        "id": "rvU3yJz6YNYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id = thread.id\n",
        ")"
      ],
      "metadata": {
        "id": "Srdgre2nWZ1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in reversed(messages.data):\n",
        "    print(message.role + \": \" + message.content[0].text.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fQF2lgWcxQ",
        "outputId": "60cb911e-4c6d-41bc-8b80-18956d15c2a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: Can you explain me how works the generator loss function?\n",
            "assistant: The generator loss function in the adversarial nets framework, specifically when both the generator (G) and discriminator (D) are multilayer perceptrons, is defined as part of a two-player minimax game with the value function \\( V(G, D) \\). The generator aims to minimize this value function, while the discriminator aims to maximize it. The value function is:\\\\\n",
            "\\[ \\min_{G} \\max_{D} V(D, G) = \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_{z}(z)}[\\log(1 - D(G(z)))] \\]\n",
            "\n",
            "Here, \\( \\mathbb{E}_{x\\sim p_{\\text{data}}(x)}[\\log D(x)] \\) represents the expectation of the log-probability that the discriminator assigns the correct label to real data (from the dataset), indicating that it came from the real data distribution. On the other hand, \\( \\mathbb{E}_{z\\sim p_{z}(z)}[\\log(1 - D(G(z)))] \\) represents the expectation of the log-probability that the discriminator assigns a correct label to fake data produced by the generator \\( G \\), indicating it came from the generator's data distribution. The generator \\( G \\) tries to generate data that is indistinguishable from real data, while the discriminator \\( D \\) tries to get better at distinguishing real data from fake data generated by \\( G \\).\n",
            "\n",
            "Instead of training the generator to minimize \\( \\log(1 - D(G(z))) \\), which can lead to a saturation problem early in learning when \\( G \\) is poor and \\( D \\) can reject samples with high confidence, it is common to train the generator to maximize \\( \\log D(G(z)) \\). This alternative objective function leads to the same fixed point of the dynamics of \\( G \\) and \\( D \\), but provides much stronger gradients early in the learning process, which can improve the learning of \\( G \\)【9†source】.\n",
            "\n",
            "This training process is implemented iteratively, with the typical procedure involving alternating between several steps of optimizing \\( D \\) and one step of optimizing \\( G \\)【9†source】.\n",
            "assistant: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnFHH8bZWeqI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}