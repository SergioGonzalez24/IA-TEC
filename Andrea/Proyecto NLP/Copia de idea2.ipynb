{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy tensorflow matplotlib language-tool-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import language_tool_python\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            CORRECTA   \n",
      "0  La puesta de sol sobre el horizonte pintaba el...  \\\n",
      "1  Después de una larga jornada de trabajo, disfr...   \n",
      "2  El aroma fresco de las flores primaverales imp...   \n",
      "3  Con cada paso, el sendero serpenteaba a través...   \n",
      "4  La melodía suave del piano llenaba la sala, cr...   \n",
      "\n",
      "                                          INCORRECTA  \n",
      "0  Puesta sol horizonte pintar cielo tonos cálido...  \n",
      "1  Después una larga jornada trabajo, disfruto re...  \n",
      "2  Aroma fresco flores primaverales impregnar jar...  \n",
      "3  Con cada paso, sendero serpenteaba través bosq...  \n",
      "4  Melodía suave piano llenar sala, creando atmós...   \n",
      "\n",
      "Index(['CORRECTA', 'INCORRECTA'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Cargar el conjunto de datos desde un archivo CSV (ajusta el nombre del archivo según sea necesario)\n",
    "df = pd.read_csv('oraciones.csv')\n",
    "\n",
    "# Mostrar las primeras filas para verificar la carga correcta\n",
    "print(df.head(), \"\\n\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para corregir la gramática de una oración\n",
    "def corregir_gramatica(oracion):\n",
    "    tool = language_tool_python.LanguageToolPublicAPI('es')\n",
    "    matches = tool.check(oracion)\n",
    "\n",
    "    # Aplicar correcciones sugeridas\n",
    "    oracion_corregida = tool.correct(oracion)\n",
    "\n",
    "    return oracion_corregida\n",
    "\n",
    "# Función para corregir la sintaxis de una oración\n",
    "def corregir_sintaxis(oracion, modelo_sintaxis, tokenizer, max_length):\n",
    "    # Preprocesar la oración\n",
    "    oracion_procesada = tokenizer.texts_to_sequences([oracion])\n",
    "    input_sequence = pad_sequences(oracion_procesada, padding='post', maxlen=max_length)\n",
    "\n",
    "    # Obtener predicciones del modelo de corrección sintáctica\n",
    "    predicciones = modelo_sintaxis.predict(input_sequence)\n",
    "\n",
    "    # Convertir las predicciones a texto\n",
    "    predicciones_texto = \" \".join([tokenizer.index_word[idx] for idx in np.argmax(predicciones[0], axis=-1)])\n",
    "\n",
    "    return predicciones_texto\n",
    "\n",
    "# Función para preprocesar datos\n",
    "def preprocesar_datos(oraciones_correctas, oraciones_incorrectas):\n",
    "    oraciones = oraciones_correctas + oraciones_incorrectas\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(oraciones)\n",
    "\n",
    "    secuencias = tokenizer.texts_to_sequences(oraciones)\n",
    "    secuencias_padded = pad_sequences(secuencias, padding='post')\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Etiquetas del conjunto de datos (1 para oraciones correctas, 0 para incorrectas)\n",
    "    etiquetas = np.concatenate([np.ones(len(oraciones_correctas)), np.zeros(len(oraciones_incorrectas))])\n",
    "\n",
    "    return secuencias_padded, etiquetas, vocab_size, tokenizer\n",
    "\n",
    "# Función para diseñar el modelo\n",
    "def diseñar_modelo(vocab_size, max_length):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dense(1, activation='sigmoid')  # Se utiliza 'sigmoid' y 1 nodo para la clasificación binaria\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def entrenar_modelo(model, input_sequences, etiquetas, epochs=10):\n",
    "    model.fit(input_sequences, etiquetas, epochs=epochs, verbose=2)\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluar_modelo(model, input_sequence, tokenizer):\n",
    "    predicciones = model.predict(input_sequence)\n",
    "\n",
    "    # Convertir predicciones a texto\n",
    "    predicciones_texto = \" \".join([tokenizer.index_word[idx] for idx in np.argmax(predicciones[0], axis=-1)])\n",
    "\n",
    "    return predicciones_texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oración a corregir\n",
    "oracion_original = \"Ayer, fui al parque a jugar con mis amigos y me olvidé mis sapatos en casa.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Oraciones para el entrenamiento\n",
    "oraciones_correctas = df['CORRECTA'].tolist()\n",
    "oraciones_incorrectas = df['INCORRECTA'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 19, 1) vs (None,)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto NLP/Copia de idea2.ipynb Celda 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m modelo_correccion_sintaxis \u001b[39m=\u001b[39m diseñar_modelo(vocab_size, max_length)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Entrenar modelo\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m entrenar_modelo(modelo_correccion_sintaxis, input_sequences, etiquetas, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Corregir gramática\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m oracion_corregida_gramatica \u001b[39m=\u001b[39m corregir_gramatica(oracion_original)\n",
      "\u001b[1;32m/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto NLP/Copia de idea2.ipynb Celda 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mentrenar_modelo\u001b[39m(model, input_sequences, etiquetas, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Andrea/Proyecto%20NLP/Copia%20de%20idea2.ipynb#X43sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     model\u001b[39m.\u001b[39mfit(input_sequences, etiquetas, epochs\u001b[39m=\u001b[39mepochs, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/y1/nsw2h6qd4l3407yyy_3z_xx00000gn/T/__autograph_generated_file4ar5djvl.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/Users/sergiogonzalez/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 19, 1) vs (None,)).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocesar datos\n",
    "input_sequences, etiquetas, vocab_size, tokenizer = preprocesar_datos(oraciones_correctas, oraciones_incorrectas)\n",
    "\n",
    "# Diseñar modelo\n",
    "max_length = len(input_sequences[0])\n",
    "modelo_correccion_sintaxis = diseñar_modelo(vocab_size, max_length)\n",
    "\n",
    "# Entrenar modelo\n",
    "entrenar_modelo(modelo_correccion_sintaxis, input_sequences, etiquetas, epochs=100)\n",
    "\n",
    "# Corregir gramática\n",
    "oracion_corregida_gramatica = corregir_gramatica(oracion_original)\n",
    "\n",
    "# Corregir sintaxis\n",
    "oracion_corregida_sintaxis = corregir_sintaxis(oracion_corregida_gramatica, modelo_correccion_sintaxis, tokenizer, max_length)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Oración Original:\", oracion_original)\n",
    "print(\"Oración Corregida Gramaticalmente:\", oracion_corregida_gramatica)\n",
    "print(\"Oración Corregida Sintácticamente:\", oracion_corregida_sintaxis)\n",
    "\n",
    "# Calcular accuracy\n",
    "accuracy = calcular_accuracy(modelo_correccion_sintaxis, input_sequences, etiquetas)\n",
    "print(\"Accuracy del modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
