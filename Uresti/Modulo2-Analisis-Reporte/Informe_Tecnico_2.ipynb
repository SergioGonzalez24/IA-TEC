{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informe Técnico - Modelo de Clasificación con Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justificación del Dataset\n",
    "\n",
    "El dataset utilizado en este proyecto es el conjunto de datos 'hotel_bookings_completo.csv', el cual contiene información sobre reservas de hoteles. A continuación, se justifica por qué este dataset es apropiado para el algoritmo de ML y para demostrar que generaliza:\n",
    "\n",
    "- **Cantidad de Datos**: El dataset contiene una cantidad significativa de datos, lo que es fundamental para entrenar modelos de ML efectivos y evitar el sobreajuste.\n",
    "\n",
    "- **Diversidad de Características**: El dataset incluye una variedad de características (atributos) relevantes para el problema de predecir si una reserva de hotel será cancelada o no. Esto permite construir modelos basados en múltiples aspectos de una reserva, lo que puede mejorar la capacidad de generalización del modelo.\n",
    "\n",
    "- **Variable Objetivo Balanceada**: La variable objetivo, que indica si una reserva fue cancelada o no, está relativamente balanceada en el dataset. Esto evita desequilibrios en las predicciones y asegura que el modelo se evalúe de manera justa.\n",
    "\n",
    "En resumen, el dataset elegido proporciona una cantidad adecuada de datos, características relevantes y una variable objetivo balanceada, lo que lo convierte en una elección apropiada para desarrollar y evaluar modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas y módulos necesarios\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos\n",
    "data = pd.read_csv(\"../Módulo2-Uso-de-framework/hotel_bookings_completo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación one-hot de variables categóricas\n",
    "data = pd.get_dummies(data, columns=[\"hotel\", \"is_canceled\", \"arrival_date_month\", \"assigned_room_type\", \"deposit_type\", \"customer_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de conjuntos de datos\n",
    "X = data.drop(\"children\", axis=1)\n",
    "y = data[\"children\"]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Explicación de la separación de conjuntos de datos\n",
    "split_explanation = {\n",
    "    \"Entrenamiento\": \"El conjunto de entrenamiento se utiliza para ajustar los parámetros del modelo y enseñar al modelo cómo se relacionan las características con la variable objetivo. El tamaño del conjunto de entrenamiento es el 80% del conjunto de datos total.\",\n",
    "    \"Validación\": \"El conjunto de validación se utiliza para ajustar hiperparámetros y evaluar el rendimiento del modelo durante el entrenamiento. Se usa para evitar el sobreajuste y garantizar que el modelo generalice bien. El tamaño del conjunto de validación es el 10% del conjunto de datos total.\",\n",
    "    \"Prueba\": \"El conjunto de prueba se utiliza para evaluar el rendimiento final del modelo después de que se haya entrenado y ajustado. Estos datos no se usan en ninguna etapa de entrenamiento o ajuste de hiperparámetros. El tamaño del conjunto de prueba es el 10% del conjunto de datos total.\"\n",
    "}\n",
    "\n",
    "# Imprimir la explicación de la separación de conjuntos de datos\n",
    "print(\"Explicación de la Separación de Conjuntos de Datos:\")\n",
    "for subset, explanation in split_explanation.items():\n",
    "    print(f\"{subset}: {explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones y evaluación en conjunto de validación\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_val, y_val_pred, average=\"weighted\")\n",
    "recall = recall_score(y_val, y_val_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_val, y_val_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.savefig(\"matriz_confusion.png\")  # Guardar la gráfica como imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un DataFrame para el informe\n",
    "informe_df = pd.DataFrame({\n",
    "    \"Métricas de Evaluación\": [\"Accuracy\", \"Precisión\", \"Recall\", \"F1-score\"],\n",
    "    \"Valor\": [accuracy, precision, recall, f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame como CSV\n",
    "informe_df.to_csv(\"informe_metricas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 3: Diagnóstico de Sesgo (Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sesgo en un modelo de aprendizaje automático se refiere a la diferencia entre el rendimiento del modelo en los datos de entrenamiento y el rendimiento en los datos de validación. El sesgo es bajo cuando el modelo se ajusta adecuadamente a los datos de entrenamiento y generaliza bien a los datos de validación. El sesgo es medio cuando hay alguna diferencia notable en el rendimiento entre los datos de entrenamiento y validación, pero no es excesivamente grande. El sesgo es alto cuando la diferencia es significativa, lo que indica que el modelo no está aprendiendo correctamente del conjunto de entrenamiento y no generaliza bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el sesgo mediante comparación de métricas de entrenamiento y validación\n",
    "train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "\n",
    "bias_diagnosis = None\n",
    "if accuracy > train_accuracy:\n",
    "    bias_diagnosis = \"Bajo\"\n",
    "elif accuracy == train_accuracy:\n",
    "    bias_diagnosis = \"Medio\"\n",
    "else:\n",
    "    bias_diagnosis = \"Alto\"\n",
    "    \n",
    "print(\"Sesgo: \" + bias_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica comparativa de Accuracy en conjuntos de entrenamiento, validación y prueba\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_values = [accuracy, 0, test_accuracy]  # Usar 0 en lugar de None para la métrica de validación\n",
    "labels = [\"Entrenamiento\", \"Validación\", \"Prueba\"]\n",
    "sns.barplot(x=labels, y=accuracy_values, palette=\"Blues\")\n",
    "plt.title(\"Comparativa de Accuracy en Conjuntos de Datos\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el sesgo mediante comparación de métricas de entrenamiento y validación\n",
    "train_pred = model.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "\n",
    "bias_diagnosis = None\n",
    "if accuracy > train_accuracy:\n",
    "    bias_diagnosis = \"Bajo\"\n",
    "elif accuracy == train_accuracy:\n",
    "    bias_diagnosis = \"Moderado\"\n",
    "else:\n",
    "    bias_diagnosis = \"Alto\"\n",
    "\n",
    "# Explicación del sesgo\n",
    "bias_explanation = {\n",
    "    \"Bajo\": \"El modelo muestra un sesgo bajo, lo que significa que se adapta bien a los datos de entrenamiento y también generaliza bien a nuevos datos en el conjunto de validación. Esto es deseable, ya que indica que el modelo no está subajustado y no falta en la representación de los datos.\",\n",
    "    \"Moderado\": \"El modelo muestra un sesgo moderado, ya que no hay una diferencia significativa en el rendimiento entre el conjunto de entrenamiento y validación. Esto sugiere que el modelo generaliza de manera razonable a nuevos datos, pero no está necesariamente sobreajustado ni subajustado.\",\n",
    "    \"Alto\": \"El modelo muestra un sesgo alto, lo que significa que hay una diferencia significativa en el rendimiento entre el conjunto de entrenamiento y validación. Esto puede indicar que el modelo se sobreajusta a los datos de entrenamiento y no generaliza bien a nuevos datos en el conjunto de validación.\"\n",
    "}\n",
    "\n",
    "# Imprimir el diagnóstico y la explicación del grado de sesgo\n",
    "print(\"Diagnóstico y Explicación del Grado de Bias o Sesgo:\")\n",
    "print(f\"Sesgo: {bias_diagnosis}\\n{bias_explanation[bias_diagnosis]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 4: Diagnóstico de Varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular la varianza mediante comparación de métricas de validación y prueba\n",
    "variance_diagnosis = None\n",
    "if accuracy > test_accuracy:\n",
    "    variance_diagnosis = \"Alto\"\n",
    "elif accuracy == test_accuracy:\n",
    "    variance_diagnosis = \"Medio\"\n",
    "else:\n",
    "    variance_diagnosis = \"Bajo\"\n",
    "    \n",
    "print(\"Varianza: \" + variance_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica comparativa de Accuracy en conjuntos de entrenamiento y validación\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_values = [accuracy, test_accuracy]  # Solo dos valores para dos conjuntos\n",
    "x_labels = [\"Entrenamiento\", \"Validación\"]\n",
    "\n",
    "plt.bar(x_labels, accuracy_values, color=\"skyblue\")\n",
    "plt.title(\"Comparativa de Accuracy en Conjuntos de Entrenamiento y Validación\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la varianza mediante comparación de métricas de validación y prueba\n",
    "variance_diagnosis = None\n",
    "if accuracy > test_accuracy:\n",
    "    variance_diagnosis = \"Alto\"\n",
    "elif accuracy == test_accuracy:\n",
    "    variance_diagnosis = \"Moderado\"\n",
    "else:\n",
    "    variance_diagnosis = \"Bajo\"\n",
    "\n",
    "# Explicación de la varianza\n",
    "variance_explanation = {\n",
    "    \"Alto\": \"El modelo muestra una varianza alta, lo que significa que hay una diferencia significativa en el rendimiento entre el conjunto de validación y prueba. Esto puede indicar que el modelo se sobreajusta a los datos de entrenamiento y no generaliza bien a nuevos datos de prueba.\",\n",
    "    \"Moderado\": \"El modelo muestra una varianza moderada, ya que no hay una diferencia significativa en el rendimiento entre el conjunto de validación y prueba. Esto sugiere una generalización razonable a nuevos datos de prueba.\",\n",
    "    \"Bajo\": \"El modelo muestra una varianza baja, lo que indica que el rendimiento en el conjunto de prueba es similar al rendimiento en el conjunto de validación. Esto es deseable, ya que el modelo generaliza de manera efectiva a nuevos datos de prueba.\"\n",
    "}\n",
    "\n",
    "# Imprimir el diagnóstico y la explicación del grado de varianza\n",
    "print(\"Diagnóstico y Explicación del Grado de Varianza:\")\n",
    "print(f\"Varianza: {variance_diagnosis}\\n{variance_explanation[variance_diagnosis]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 5: Nivel de Ajuste del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica comparativa de Accuracy en conjuntos de entrenamiento, validación y prueba\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracy_values = [accuracy, 0, test_accuracy]  # Usar 0 en lugar de None para la métrica de validación\n",
    "labels = [\"Entrenamiento\", \"Validación\", \"Prueba\"]\n",
    "\n",
    "sns.barplot(x=labels, y=accuracy_values, palette=\"Blues\")\n",
    "plt.title(\"Comparativa de Accuracy en Conjuntos de Datos\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicación del nivel de ajuste del modelo\n",
    "if fit_level == \"Buen Ajuste\":\n",
    "    fit_explanation = \"El modelo muestra un buen ajuste ya que tiene un sesgo bajo y una varianza baja, lo que indica que se adapta bien a los datos de entrenamiento y generaliza bien a nuevos datos.\"\n",
    "elif fit_level == \"Ajuste Adecuado\":\n",
    "    fit_explanation = \"El modelo muestra un ajuste adecuado ya que tiene un sesgo adecuado y una varianza adecuada, lo que indica una generalización razonable sin problemas de sobreajuste ni subajuste.\"\n",
    "else:\n",
    "    fit_explanation = \"El modelo muestra un sobreajuste, ya que tiene un sesgo bajo pero una varianza alta, lo que indica que se adapta demasiado a los datos de entrenamiento y no generaliza bien a nuevos datos.\"\n",
    "\n",
    "# Imprimir el diagnóstico y la explicación del nivel de ajuste del modelo\n",
    "print(\"Diagnóstico y Explicación del Nivel de Ajuste del Modelo:\")\n",
    "print(f\"Sesgo: {bias_diagnosis}\\n{bias_explanation}\")\n",
    "print(f\"Varianza: {variance_diagnosis}\\n{variance_explanation}\")\n",
    "print(f\"Nivel de Ajuste del Modelo: {fit_level}\\n{fit_explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 6: Técnicas de Mejora del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, se presentarán tres técnicas de mejora del modelo, junto con el código y las gráficas que demuestran su efecto en el desempeño del modelo.\n",
    "\n",
    "### Técnica 1: Codificación One-Hot\n",
    "\n",
    "**¿Qué es la codificación one-hot?**\n",
    "\n",
    "La codificación one-hot es una técnica utilizada para tratar con variables categóricas en el aprendizaje automático. Las variables categóricas son aquellas que representan categorías o etiquetas, pero no tienen una relación numérica inherente. Por ejemplo, el tipo de habitación de un hotel (por ejemplo, \"individual\", \"doble\", \"suite\") es una variable categórica.\n",
    "\n",
    "La codificación one-hot convierte una variable categórica en un conjunto de variables binarias (0 o 1), donde cada variable representa una categoría única. Si tenemos N categorías en una variable categórica, se crearán N nuevas variables binarias, una para cada categoría. Estas variables binarias indican si la observación pertenece o no a cada categoría.\n",
    "\n",
    "**Efecto de la codificación one-hot en el modelo:**\n",
    "\n",
    "Antes de aplicar la codificación one-hot, las variables categóricas suelen codificarse con números enteros. Por ejemplo, el tipo de habitación podría codificarse como 1 para \"individual\", 2 para \"doble\" y 3 para \"suite\". Sin embargo, esta codificación podría llevar al modelo a malinterpretar la relación numérica entre las categorías. La codificación one-hot resuelve este problema, ya que crea variables binarias independientes para cada categoría, lo que permite al modelo capturar mejor las diferencias entre las categorías sin asumir relaciones numéricas que no existen.\n",
    "\n",
    "**Comparativo antes y después de aplicar la codificación one-hot:**\n",
    "\n",
    "Antes de aplicar la codificación one-hot, las variables categóricas se codifican como enteros, como se mencionó anteriormente. Esto puede llevar a un desempeño deficiente del modelo, especialmente cuando las categorías no tienen un orden intrínseco. Las suposiciones sobre relaciones numéricas pueden llevar a errores de interpretación.\n",
    "\n",
    "Después de aplicar la codificación one-hot, cada categoría se convierte en una variable binaria independiente. Esto significa que cada observación se representa claramente como perteneciente o no a cada categoría. El modelo puede usar esta información de manera más efectiva para tomar decisiones precisas.\n",
    "\n",
    "La mejora en el desempeño del modelo después de aplicar la codificación one-hot suele ser evidente en métricas como la precisión, el recall y el F1-score, especialmente cuando se trata de variables categóricas importantes para el problema.\n",
    "\n",
    "En resumen, la codificación one-hot es una técnica fundamental para manejar variables categóricas y suele mejorar el desempeño del modelo al evitar suposiciones incorrectas sobre relaciones numéricas entre categorías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Técnica 2: Ajuste de Hiperparámetros\n",
    "\n",
    "**¿Qué son los hiperparámetros?**\n",
    "\n",
    "Los hiperparámetros son configuraciones ajustables del modelo que no se aprenden automáticamente a partir de los datos, a diferencia de los parámetros del modelo, que se ajustan durante el entrenamiento. Los hiperparámetros incluyen cosas como la profundidad máxima de un árbol de decisión, el número de estimadores en un modelo de bosque aleatorio o la tasa de aprendizaje en un algoritmo de gradiente descendente.\n",
    "\n",
    "**Efecto del ajuste de hiperparámetros en el modelo:**\n",
    "\n",
    "El ajuste de hiperparámetros es una técnica crucial para optimizar el rendimiento del modelo. Un conjunto inadecuado de hiperparámetros puede llevar a un modelo subajustado o sobreajustado. El proceso de búsqueda de hiperparámetros implica probar diferentes combinaciones de valores para los hiperparámetros y evaluar cómo afectan al rendimiento del modelo.\n",
    "\n",
    "**Comparativo antes y después del ajuste de hiperparámetros:**\n",
    "\n",
    "Antes del ajuste de hiperparámetros, el modelo se entrena con valores predeterminados o aleatorios para los hiperparámetros. Esto puede llevar a un modelo que no esté optimizado para el problema específico que se está abordando. Por lo tanto, es probable que el desempeño del modelo sea subóptimo.\n",
    "\n",
    "Después del ajuste de hiperparámetros, se realizan búsquedas sistemáticas para encontrar la combinación de hiperparámetros que maximiza el rendimiento del modelo en un conjunto de datos de validación. Esto a menudo resulta en un modelo más preciso y generalizado.\n",
    "\n",
    "La mejora en el desempeño del modelo después del ajuste de hiperparámetros puede observarse en métricas como la precisión, el recall y el F1-score, que tienden a ser mejores en el modelo ajustado.\n",
    "\n",
    "#### Técnica 3: Evaluación y Ajuste\n",
    "\n",
    "**¿En qué consiste la evaluación y ajuste?**\n",
    "\n",
    "La tercera técnica implica una evaluación más profunda del modelo después de que se haya entrenado y ajustado inicialmente. Esto incluye una revisión exhaustiva de métricas de evaluación como precisión, recall, F1-score y otras métricas relevantes para el problema. También se consideran matrices de confusión y curvas de ROC si es aplicable. Además, se exploran posibles ajustes adicionales del modelo, como la selección de características, la ponderación de clases o la optimización de umbrales de decisión.\n",
    "\n",
    "**Efecto de la evaluación y ajuste en el modelo:**\n",
    "\n",
    "La evaluación y el ajuste detallados son esenciales para asegurar que el modelo sea lo más efectivo posible en la tarea que se le asigna. Esto permite detectar posibles problemas de sesgo o varianza, así como oportunidades para mejorar el desempeño general.\n",
    "\n",
    "**Comparativo antes y después de la evaluación y ajuste:**\n",
    "\n",
    "Antes de la evaluación y ajuste, el modelo se evalúa principalmente en términos de métricas estándar, pero no se profundiza en detalles adicionales ni se consideran ajustes específicos. Esto puede resultar en un modelo que podría beneficiarse de refinamientos adicionales.\n",
    "\n",
    "Después de la evaluación y el ajuste, el modelo se beneficia de mejoras adicionales que pueden aumentar su precisión, recall y rendimiento general. Los ajustes realizados durante esta fase se basan en un análisis detallado de las métricas y la comprensión más profunda del comportamiento del modelo.\n",
    "\n",
    "En resumen, las Técnicas 2 y 3 son cruciales para optimizar el desempeño del modelo, y sus efectos se reflejarán en una mejora en las métricas de evaluación después de su aplicación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de 1 tecnica de Mejorado de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El código realiza un ajuste de hiperparámetros mediante búsqueda de cuadrícula con validación\n",
    "# cruzada.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definición de hiperparámetros para la búsqueda de cuadrícula\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Crear un clasificador RandomForest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Realizar la búsqueda de cuadrícula\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Entrenar el mejor modelo en los datos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con el modelo ajustado\n",
    "y_val_pred_after = best_model.predict(X_val)\n",
    "y_test_pred_after = best_model.predict(X_test)\n",
    "\n",
    "# Calcular métricas antes y después del ajuste\n",
    "accuracy_before = accuracy\n",
    "accuracy_after_val = accuracy_score(y_val, y_val_pred_after)\n",
    "accuracy_after_test = accuracy_score(y_test, y_test_pred_after)\n",
    "\n",
    "# Imprimir comparativos antes y después del ajuste de hiperparámetros\n",
    "print(\"Comparativo antes y después del Ajuste de Hiperparámetros:\")\n",
    "print(f\"Accuracy en Validación (Antes): {accuracy:.4f}\")\n",
    "print(f\"Accuracy en Validación (Después): {accuracy_after_val:.4f}\")\n",
    "print(f\"Accuracy en Prueba (Antes): {test_accuracy:.4f}\")\n",
    "print(f\"Accuracy en Prueba (Después): {accuracy_after_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión del DataFrame\n",
    "informe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión de las métricas de evaluación en el conjunto de prueba después de aplicar las técnicas de mejora\n",
    "print(\"\\nMétricas de Evaluación en el Conjunto de Prueba (Después de Mejoras):\")\n",
    "print(f\"Accuracy en el conjunto de prueba: {test_accuracy}\")\n",
    "print(f\"Precision en el conjunto de prueba: {precision}\")\n",
    "print(f\"Recall en el conjunto de prueba: {recall}\")\n",
    "print(f\"F1-score en el conjunto de prueba: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
