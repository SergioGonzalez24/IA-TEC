{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aop56fGWwtQn"
   },
   "source": [
    "# Python Pandas Tutorial: A Complete Introduction for Beginners\n",
    "\n",
    "This tutorial was originally written by [George McIntire](https://www.learndatasci.com/author/GeorgeMcIntire), [Brendan Martin](https://www.learndatasci.com/author/brendan) and [Lauren Washington](https://www.learndatasci.com/author/LaurenWashington) for [LearnDataSci.com](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/)\n",
    "\n",
    "This version has been repurposed and expanded further by Julio Arriaga for TC3006C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLBVnpZGwtQ2"
   },
   "source": [
    "The *pandas* package is the most important tool at the disposal of Data Scientists and Analysts working in Python today. The powerful machine learning and glamorous visualization tools may get all the attention, but pandas is the backbone of most data projects.\n",
    "\n",
    ">\\[*pandas*\\] is derived from the term \"**pan**el **da**ta\", an econometrics term for data sets that include observations over multiple time periods for the same individuals. — [Wikipedia](https://en.wikipedia.org/wiki/Pandas_%28software%29)\n",
    "\n",
    "In this tutorial, we will go over the essential bits of information about pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gukap-mJwtQ6"
   },
   "source": [
    "<img src=\"https://storage.googleapis.com/lds-media/images/the-rise-in-popularity-of-pandas.width-1200.png\" width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wxx-lnkQwtQ8"
   },
   "source": [
    "## What's Pandas for?\n",
    "\n",
    "Pandas has so many uses that it might make sense to list the things it can't do instead of what it can do.\n",
    "\n",
    "This tool is essentially your data’s home. Through pandas, you get acquainted with your data by cleaning, transforming, and analyzing it.\n",
    "\n",
    "For example, say you want to explore a dataset stored in a CSV on your computer. Pandas will extract the data from that CSV into a DataFrame — a table, basically — then let you do things like:\n",
    "\n",
    "- Calculate statistics and answer questions about the data, like\n",
    "\n",
    "\n",
    "    - What's the average, median, max, or min of each column?\n",
    "    - Does column A correlate with column B?\n",
    "    - What does the distribution of data in column C look like?\n",
    "\n",
    "\n",
    "- Clean the data by doing things like removing missing values and filtering rows or columns by some criteria\n",
    "\n",
    "\n",
    "- Visualize the data with help from Matplotlib. Plot bars, lines, histograms, bubbles, and more.\n",
    "\n",
    "\n",
    "- Store the cleaned, transformed data back into a CSV, other file or database\n",
    "\n",
    "\n",
    "Before you jump into the modeling or the complex visualizations you need to have a good understanding of the nature of your dataset and pandas is the best avenue through which to do that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C42qLdXuwtQ_"
   },
   "source": [
    "## How does pandas fit into the data science toolkit?\n",
    "\n",
    "Not only is the pandas library a central component of the data science toolkit but it is used in conjunction with other libraries in that collection.\n",
    "\n",
    "Pandas is built on top of the **NumPy** package, meaning a lot of the structure of NumPy is used or replicated in Pandas. Data in pandas is often used to feed statistical analysis in **SciPy**, plotting functions from **Matplotlib**, and machine learning algorithms in **Scikit-learn**.\n",
    "\n",
    "Jupyter Notebooks offer a good environment for using pandas to do data exploration and modeling, but pandas can also be used in text editors just as easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO08R7XEwtRC"
   },
   "source": [
    "## First Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pjeUohtNwtRW",
    "ExecuteTime": {
     "end_time": "2023-08-17T23:50:34.557402Z",
     "start_time": "2023-08-17T23:50:34.553204Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect To Google drive\n",
    "Before we start, we will connect this notebook to our Google drive account. This will enable us to pull data directly from our account, instead of having to upload test data each time."
   ],
   "metadata": {
    "id": "KVV7mT4oz7qe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "!pwd  # show current path"
   ],
   "metadata": {
    "id": "q8kYO1VGz-_C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Move to the folder where your data set is stored. Change the path to your local drive where the datasets are stored."
   ],
   "metadata": {
    "id": "rEmROYli0MbL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%cd \"/content/gdrive/MyDrive/...path to your folder.../\"\n",
    "!ls  # show current directory"
   ],
   "metadata": {
    "id": "ARM_s__P0QK5",
    "ExecuteTime": {
     "end_time": "2023-08-17T23:52:28.708794Z",
     "start_time": "2023-08-17T23:52:28.583275Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './Julio/train.csv'\n",
      "/Users/sergiogonzalez/Documents/GitHub/IA-TEC/Julio\n",
      "train.csv             tutorial_numpy.ipynb  tutorial_pandas.ipynb\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KitXYxCKwtRX"
   },
   "source": [
    "Now to the basic components of pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEuq4UbfwtRb"
   },
   "source": [
    "## Core components of pandas: Series and DataFrames\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`.\n",
    "\n",
    "A `Series` is essentially a column, and a `DataFrame` is a multi-dimensional table made up of a collection of Series.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/series-and-dataframe.width-1200.png\" width=600px />\n",
    "\n",
    "More formally, `Series` is a one-dimensional labeled array capable of holding any data type (integers, strings, floating point numbers, Python objects, etc.). The axis labels are collectively referred to as the index. `Series` acts very similarly to a `ndarray`, and is a valid argument to most NumPy functions.\n",
    "\n",
    "`DataFrame` is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dict of `Series` objects. It is generally the most commonly used pandas object.\n",
    "\n",
    "DataFrames and Series are quite similar in that many operations that you can do with one you can do with the other, such as filling in null values and calculating the mean.\n",
    "\n",
    "You'll see how these components work when we start working with data below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oppo0gwwtRe"
   },
   "source": [
    "### Creating DataFrames from scratch\n",
    "Creating DataFrames right in Python is good to know and quite useful when testing new methods and functions you find in the pandas docs.\n",
    "\n",
    "There are *many* ways to create a DataFrame from scratch, but a great option is to just use a simple `dict`.\n",
    "\n",
    "Let's say we have a fruit stand that sells apples and oranges. We want to have a column for each fruit and a row for each customer purchase. To organize this as a dictionary for pandas we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yatEBD3QwtRf"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'apples': [3, 2, 0, 1],\n",
    "    'oranges': [0, 3, 7, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTrboZaGwtRg"
   },
   "source": [
    "And then pass it to the pandas DataFrame constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JUCNpbKwtRh"
   },
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data)\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE_b3FyOwtRj"
   },
   "source": [
    "**How did that work?**\n",
    "\n",
    "Each *(key, value)* item in `data` corresponds to a *column* in the resulting DataFrame.\n",
    "\n",
    "The **Index** of this DataFrame was given to us on creation as the numbers 0-3, but we could also create our own when we initialize the DataFrame.\n",
    "\n",
    "Let's have customer names as our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ol6nS4pXwtRl"
   },
   "outputs": [],
   "source": [
    "purchases = pd.DataFrame(data, index=['June', 'Robert', 'Lily', 'David'])\n",
    "\n",
    "purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3UzTIOCwtRn"
   },
   "source": [
    "So now we could **loc**ate a customer's order by using their name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxM4YzI_wtRq"
   },
   "outputs": [],
   "source": [
    "purchases.loc['June']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As previously mentioned, each column of a dataframe is defined as a `Series` object, these objects are built on top of, and are very similar to, numpy arrays. You can even convert them directly using `to_numpy()`"
   ],
   "metadata": {
    "id": "TSXZkqkN4HIi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(type(purchases[\"apples\"]))\n",
    "array = purchases[\"apples\"].to_numpy()\n",
    "print(type(array))\n",
    "print(array)"
   ],
   "metadata": {
    "id": "NMS2dOFP4F6u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also make new columns with indicators, since dataframes are based on numpy arrays, you can use numpy operations across full arrays."
   ],
   "metadata": {
    "id": "VwiBjZ0t5oCt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "purchases[\"total\"] = purchases[\"apples\"] + purchases[\"oranges\"]\n",
    "purchases.head()"
   ],
   "metadata": {
    "id": "1gtO55uv5w9o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5aoRJnCwtRt"
   },
   "source": [
    "There's more on locating and extracting data from the DataFrame later, but now you should be able to create a DataFrame with any random data to learn on.\n",
    "\n",
    "Let's move on to some quick methods for creating DataFrames from various other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojmz-berwtRu"
   },
   "source": [
    "## How to read in data\n",
    "\n",
    "It’s quite simple to load data from various file formats into a DataFrame. In the following examples we'll keep using our apples and oranges data, but this time it's coming from various files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uml6US_FwtRv"
   },
   "source": [
    "### Reading data from CSVs\n",
    "\n",
    "With CSV files all you need is a single line to load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbjzsHcvwtRw"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO4KonhhwtRx"
   },
   "source": [
    "CSVs don't have indexes like our DataFrames, so all we need to do is just designate the `index_col` when reading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsC3yLJmwtRx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('purchases.csv', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UTX-pwOwtRy"
   },
   "source": [
    "Here we're setting the index to be column zero.\n",
    "\n",
    "You'll find that most CSVs won't ever have an index column and so usually you don't have to worry about this step.\n",
    "\n",
    "### Reading data from JSON\n",
    "\n",
    "If you have a JSON file — which is essentially a stored Python `dict` — pandas can read this just as easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cd8zRhC9wtRz"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('purchases.json')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAN2niXlwtR0"
   },
   "source": [
    "Notice this time our index came with us correctly since using JSON allowed indexes to work through nesting. Feel free to open `data_file.json` in a notepad so you can see how it works.\n",
    "\n",
    "Pandas will try to figure out how to create a DataFrame by analyzing structure of your JSON, and sometimes it doesn't get it right. Often you'll need to set the `orient` keyword argument depending on the structure, so check out [read_json docs](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) about that argument to see which orientation you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I065CcCWwtSC"
   },
   "source": [
    "We can also set an index after-the-fact using `set_index()` on *any* DataFrame using *any* column at *any* time. Indexing Series and DataFrames is a very common task, and the different ways of doing it is worth remembering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcvmD46vwtSD"
   },
   "source": [
    "### Converting back to a CSV or JSON\n",
    "\n",
    "So after extensive work on cleaning your data, you’re now ready to save it as a file of your choice. Similar to the ways we read in data, pandas provides intuitive commands to save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL6DCJq5wtSE"
   },
   "outputs": [],
   "source": [
    "df.to_csv('new_purchases.csv')\n",
    "\n",
    "df.to_json('new_purchases.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P10r_KKLwtSF"
   },
   "source": [
    "When we save JSON and CSV files, all we have to input into those functions is our desired filename with the appropriate file extension.\n",
    "\n",
    "Let's move on to importing some real-world data and detailing a few of the operations you'll be using a lot.\n",
    "\n",
    "\n",
    "## Most important DataFrame operations\n",
    "\n",
    "DataFrames possess hundreds of methods and other operations that are crucial to any analysis. As a beginner, you should know the operations that perform simple transformations of your data and those that provide fundamental statistical analysis.\n",
    "\n",
    "Let's load in the IMDB movies dataset to begin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUvJoGkbwtSH"
   },
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv(\"IMDB-Movie-Data.csv\", index_col=\"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDQFNuKKwtSI"
   },
   "source": [
    "We're loading this dataset from a CSV and designating the movie titles to be our index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXkJzHDFwtSJ"
   },
   "source": [
    "### Viewing your data\n",
    "\n",
    "The first thing to do when opening a new dataset is print out a few rows to keep as a visual reference. We accomplish this with `.head()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEoyhF1YwtSJ"
   },
   "outputs": [],
   "source": [
    "movies_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_leDXNYywtSL"
   },
   "source": [
    "`.head()` outputs the **first** five rows of your DataFrame by default, but we could also pass a number as well: `movies_df.head(10)` would output the top ten rows, for example.\n",
    "\n",
    "To see the **last** five rows use `.tail()`. `tail()` also accepts a number, and in this case we printing the bottom two rows.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nZhy9M-wtSN"
   },
   "outputs": [],
   "source": [
    "movies_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HemV0ql5wtSO"
   },
   "source": [
    "Typically when we load in a dataset, we like to view the first five or so rows to see what's under the hood. Here we can see the names of each column, the index, and examples of values in each row.\n",
    "\n",
    "You'll notice that the index in our DataFrame is the *Title* column, which you can tell by how the word *Title* is slightly lower than the rest of the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVsOM_FNwtSP"
   },
   "source": [
    "### Getting info about your data\n",
    "\n",
    "`.info()` should be one of the very first commands you run after loading your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI13Z8nYwtSP"
   },
   "outputs": [],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khv9BDpywtSR"
   },
   "source": [
    "`.info()` provides the essential details about your dataset, such as the number of rows and columns, the number of non-null values, what type of data is in each column, and how much memory your DataFrame is using.\n",
    "\n",
    "Notice in our movies dataset we have some obvious missing values in the `Revenue` and `Metascore` columns. We'll look at how to handle those in a bit.\n",
    "\n",
    "Seeing the datatype quickly is actually quite useful. Imagine you just imported some JSON and the integers were recorded as strings. You go to do some arithmetic and find an \"unsupported operand\" Exception because you can't do math with strings. Calling `.info()` will quickly point out that your column you thought was all integers are actually string objects.\n",
    "\n",
    "Another fast and useful attribute is `.shape`, which outputs just a tuple of (rows, columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eWdXbD4wtSR"
   },
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cslMEEMwtSS"
   },
   "source": [
    "Note that `.shape` has no parentheses and is a simple tuple of format (rows, columns). So we have **1000 rows** and **11 columns** in our movies DataFrame.\n",
    "\n",
    "You'll be going to `.shape` a lot when cleaning and transforming data. For example, you might filter some rows based on some criteria and then want to know quickly how many rows were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxB7XIANwtST"
   },
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk_zNpeAwtST"
   },
   "source": [
    "This dataset does not have duplicate rows, but it is always important to verify you aren't aggregating duplicate rows.\n",
    "\n",
    "To demonstrate, let's simply just double up our movies DataFrame by appending it to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6KxZQnmwtSW"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.concat([movies_df, movies_df])\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JaB4mz1WwtSa"
   },
   "source": [
    "Using `concat()` will return a copy without affecting the original DataFrame. We are capturing this copy in `temp` so we aren't working with the real data.\n",
    "\n",
    "Notice call `.shape` quickly proves our DataFrame rows have doubled.\n",
    "\n",
    "Now we can try dropping duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpTLH9wjwtSb"
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.drop_duplicates()\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhucymx0wtSd"
   },
   "source": [
    "Just like `concat()`, the `drop_duplicates()` method will also return a copy of your DataFrame, but this time with duplicates removed. Calling `.shape` confirms we're back to the 1000 rows of our original dataset.\n",
    "\n",
    "It's a little verbose to keep assigning DataFrames to the same variable like in this example. For this reason, pandas has the `inplace` keyword argument on many of its methods. Using `inplace=True` will modify the DataFrame object in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1hSeTnZwtSk"
   },
   "outputs": [],
   "source": [
    "temp_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_6AeghLwtSm"
   },
   "source": [
    "Now our `temp_df` *will* have the transformed data automatically.\n",
    "\n",
    "Another important argument for `drop_duplicates()` is `keep`, which has three possible options:\n",
    "\n",
    "* `first`: (default) Drop duplicates except for the first occurrence.\n",
    "* `last`: Drop duplicates except for the last occurrence.\n",
    "* `False`: Drop all duplicates.\n",
    "\n",
    "Since we didn't define the `keep` arugment in the previous example it was defaulted to `first`. This means that if two rows are the same pandas will drop the second row and keep the first row. Using `last` has the opposite effect: the first row is dropped.\n",
    "\n",
    "`False`, on the other hand, will drop all duplicates. If two rows are the same then both will be dropped. Watch what happens to `temp_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QP2HfBFawtSn"
   },
   "outputs": [],
   "source": [
    "temp_df = pd.concat([movies_df, movies_df])  # make a new copy\n",
    "\n",
    "temp_df.drop_duplicates(inplace=True, keep=False)\n",
    "\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZdzqiOXwtSp"
   },
   "source": [
    "Since all rows were duplicates, `keep=False` dropped them all resulting in zero rows being left over. If you're wondering why you would want to do this, one reason is that it allows you to locate all duplicates in your dataset. When conditional selections are shown below you'll see how to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLwbAdYTwtSq"
   },
   "source": [
    "### Column cleanup\n",
    "\n",
    "Many times datasets will have verbose column names with symbols, upper and lowercase words, spaces, and typos. To make selecting data by column name easier we can spend a little time cleaning up their names.\n",
    "\n",
    "Here's how to print the column names of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VozPlxpuwtSr"
   },
   "outputs": [],
   "source": [
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gVHAKpzwtSs"
   },
   "source": [
    "Not only does `.columns` come in handy if you want to rename columns by allowing for simple copy and paste, it's also useful if you need to understand why you are receiving a `Key Error` when selecting data by column.\n",
    "\n",
    "We can use the `.rename()` method to rename certain or all columns via a `dict`. We don't want parentheses, so let's rename those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfWhAsEHwtSs"
   },
   "outputs": [],
   "source": [
    "movies_df.rename(columns={\n",
    "        'Runtime (Minutes)': 'Runtime',\n",
    "        'Revenue (Millions)': 'Revenue_millions'\n",
    "    }, inplace=True)\n",
    "\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG_EJeQmwtSu"
   },
   "source": [
    "Excellent. But what if we want to lowercase all names? Instead of using `.rename()` we could also set a list of names to the columns like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFbVBk9QwtSv"
   },
   "outputs": [],
   "source": [
    "movies_df.columns = ['rank', 'genre', 'description', 'director', 'actors', 'year', 'runtime',\n",
    "                     'rating', 'votes', 'revenue_millions', 'metascore']\n",
    "\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrqZGPMOwtSx"
   },
   "source": [
    "But that's too much work. Instead of just renaming each column manually we can do a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgGDbphnwtSx"
   },
   "outputs": [],
   "source": [
    "movies_df.columns = [col.lower() for col in movies_df]\n",
    "\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3VKFsKrwtSy"
   },
   "source": [
    "`list` (and `dict`) comprehensions come in handy a lot when working with pandas and data in general.\n",
    "\n",
    "It's a good idea to lowercase, remove special characters, and replace spaces with underscores if you'll be working with a dataset for some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inWx9cq2wtSz"
   },
   "source": [
    "### How to work with missing values\n",
    "\n",
    "When exploring data, you’ll most likely encounter missing or null values, which are essentially placeholders for non-existent values. Most commonly you'll see Python's `None` or NumPy's `np.nan`, each of which are handled differently in some situations.\n",
    "\n",
    "There are two options in dealing with nulls:\n",
    "\n",
    "1. Get rid of rows or columns with nulls\n",
    "2. Replace nulls with non-null values, a technique known as **imputation**\n",
    "\n",
    "Let's calculate to total number of nulls in each column of our dataset. The first step is to check which cells in our DataFrame are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poyqe33CwtS0"
   },
   "outputs": [],
   "source": [
    "movies_df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZqHPgPiwtS2"
   },
   "source": [
    "Notice `isnull()` returns a DataFrame where each cell is either True or False depending on that cell's null status.\n",
    "\n",
    "To count the number of nulls in each column we use an aggregate function for summing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3mD4Io8wtS2"
   },
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQEDPDqowtS4"
   },
   "source": [
    "`.isnull()` just by iteself isn't very useful, and is usually used in conjunction with other methods, like `sum()`.\n",
    "\n",
    "We can see now that our data has **128** missing values for `revenue_millions` and **64** missing values for `metascore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Tg36wpAwtS7"
   },
   "source": [
    "#### Removing null values\n",
    "\n",
    "Data Scientists and Analysts regularly face the dilemma of dropping or imputing null values, and is a decision that requires intimate knowledge of your data and its context. Overall, removing null data is only suggested if you have a small amount of missing data.\n",
    "\n",
    "Remove nulls is pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "iMhXIqshwtS9"
   },
   "outputs": [],
   "source": [
    "movies_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTjLIocrwtTF"
   },
   "source": [
    "This operation will delete any **row** with at least a single null value, but it will return a new DataFrame without altering the original one. You could specify `inplace=True` in this method as well.\n",
    "\n",
    "So in the case of our dataset, this operation would remove 128 rows where `revenue_millions` is null and 64 rows where `metascore` is null. This obviously seems like a waste since there's perfectly good data in the other columns of those dropped rows. That's why we'll look at imputation next.\n",
    "\n",
    "Other than just dropping rows, you can also drop columns with null values by setting `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "7PG3b6iywtTF"
   },
   "outputs": [],
   "source": [
    "movies_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTGcXA3owtTM"
   },
   "source": [
    "In our dataset, this operation would drop the `revenue_millions` and `metascore` columns.\n",
    "\n",
    "**Intuition side note**: What's with this `axis=1` parameter?\n",
    "\n",
    "It's not immediately obvious where `axis` comes from and why you need it to be 1 for it to affect columns. To see why, just look at the `.shape` output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEmY1W1BwtTO"
   },
   "outputs": [],
   "source": [
    "movies_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yfhY_eWwtTZ"
   },
   "source": [
    "As we learned above, this is a tuple that represents the shape of the DataFrame, i.e. 1000 rows and 11 columns. Note that the *rows* are at index zero of this tuple and *columns* are at **index one** of this tuple. This is why `axis=1` affects columns. This comes from NumPy, and is a great example of why learning NumPy is worth your time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGU1xzZEwtTb"
   },
   "source": [
    "### Imputation\n",
    "\n",
    "Imputation is a conventional feature engineering technique used to keep valuable data that have null values.\n",
    "\n",
    "There may be instances where dropping every row with a null value removes too big a chunk from your dataset, so instead we can impute that null with another value, usually the **mean** or the **median** of that column.\n",
    "\n",
    "Let's look at imputing the missing values in the `revenue_millions` column. First we'll extract that column into its own variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZLNMCGfwtTf"
   },
   "outputs": [],
   "source": [
    "revenue = movies_df['revenue_millions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF-2JHOowtTg"
   },
   "source": [
    "Using square brackets is the general way we select columns in a DataFrame.\n",
    "\n",
    "If you remember back to when we created DataFrames from scratch, the keys of the `dict` ended up as column names. Now when we select columns of a DataFrame, we use brackets just like if we were accessing a Python dictionary.\n",
    "\n",
    "`revenue` now contains a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGoXnd4twtTg"
   },
   "outputs": [],
   "source": [
    "revenue.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR5ACFL4wtTh"
   },
   "source": [
    "Slightly different formatting than a DataFrame, but we still have our `Title` index.\n",
    "\n",
    "We'll impute the missing values of revenue using the mean. Here's the mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybjyj7xCwtTh"
   },
   "outputs": [],
   "source": [
    "revenue_mean = revenue.mean()\n",
    "\n",
    "revenue_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQvlCJHNwtUZ"
   },
   "source": [
    "With the mean, let's fill the nulls using `fillna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGDcMa7RwtUb"
   },
   "outputs": [],
   "source": [
    "revenue.fillna(revenue_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtXwiXrTwtUc"
   },
   "source": [
    "We have now replaced all nulls in `revenue` with the mean of the column. Notice that by using `inplace=True` we have actually affected the original `movies_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wox-5N5RwtUd"
   },
   "outputs": [],
   "source": [
    "movies_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB2zMvAZwtUi"
   },
   "source": [
    "Imputing an entire column with the same value like this is a basic example. It would be a better idea to try a more granular imputation by Genre or Director.\n",
    "\n",
    "For example, you would find the mean of the revenue generated in each genre individually and impute the nulls in each genre with that genre's mean.\n",
    "\n",
    "Let's now look at more ways to examine and understand the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYVEzaG0wtUi"
   },
   "source": [
    "### Understanding your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG_s8YN8wtUj"
   },
   "source": [
    "Using `describe()` on an entire DataFrame we can get a summary of the distribution of continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Txg5JRwqwtUj"
   },
   "outputs": [],
   "source": [
    "movies_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xyv7SeipwtUk"
   },
   "source": [
    "Understanding which numbers are continuous also comes in handy when thinking about the type of plot to use to represent your data visually.\n",
    "\n",
    "`.describe()` can also be used on a categorical variable to get the count of rows, unique count of categories, top category, and freq of top category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3GPVab8wtUl"
   },
   "outputs": [],
   "source": [
    "movies_df['genre'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5OUMFZ6wtUm"
   },
   "source": [
    "This tells us that the genre column has 207 unique values, the top value is Action/Adventure/Sci-Fi, which shows up 50 times (freq).\n",
    "\n",
    "`.value_counts()` can tell us the frequency of all values in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpcNBqM6wtUm"
   },
   "outputs": [],
   "source": [
    "movies_df['genre'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding Categorical Values\n",
    "\n",
    "Encoding categorical values is recommended in data analysis and machine learning for several reasons. Many machine learning algorithms and statistical models require numerical inputs. Categorical variables, which represent qualitative data, need to be transformed into numerical values to be used effectively in these algorithms. Additionally, if categorical data is left in its original form without encoding, some algorithms might misinterpret the data as ordinal (having a meaningful order) when there's actually no inherent order. For example, if you have categories like \"Red,\" \"Blue,\" and \"Green,\" encoding them as 0, 1, and 2 could wrongly imply an ordinal relationship, leading to incorrect interpretations. Finally, encodings can also help save resources (numerical values take up less space than text labels), plus they can also assist in handling missing values.\n",
    "\n",
    "There are several methods for encoding categorical variables, each with its own advantages and considerations. Common encoding techniques include one-hot encoding (creating binary columns for each category), label encoding (assigning unique integers to categories), and target encoding (using target variable information to encode categories).\n",
    "\n",
    "For our movie data, it would be advisable to encode the `genres` column into categorical values. Notice that since we have multiple possible combinations of genres, directly encoding the column would result in over 200 dummy variables."
   ],
   "metadata": {
    "id": "7Lvu90THRaE3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df['genre'].describe()"
   ],
   "metadata": {
    "id": "qwk_oYpEW-zD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead, we'll need to split the genre into individual values and create only one dummy binary variable for each genre. We can use the `get_dummies` function of pandas' `str` module to achieve this."
   ],
   "metadata": {
    "id": "PGkzMktSXFIW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, create binary columns for each genre."
   ],
   "metadata": {
    "id": "1NovE1zkXxfe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "genre_dummies = movies_df[\"genre\"].str.get_dummies(\",\")"
   ],
   "metadata": {
    "id": "8U8nvI5JRksr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then concatenate the original dataframe with the new binary columns"
   ],
   "metadata": {
    "id": "UBS3YMxrX_Nf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df = pd.concat([movies_df, genre_dummies], axis=1)"
   ],
   "metadata": {
    "id": "a3iZb8eMX_q7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Last, drop the original `genre` column."
   ],
   "metadata": {
    "id": "q_7QpdvAYAHW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.drop('genre', axis=1, inplace=True)"
   ],
   "metadata": {
    "id": "MPbxPqUOYPv3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now every genre has it's own binary column. We could repeat this process for the `actors` column."
   ],
   "metadata": {
    "id": "ZKBTprKeYbka"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "movies_df.head()"
   ],
   "metadata": {
    "id": "1XKVJl03YqPw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-Hx9uxWwtUn"
   },
   "source": [
    "#### Relationships between continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC8GCFT9wtUn"
   },
   "source": [
    "By using the correlation method `.corr()` we can generate the relationship between each continuous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wRpYtj1wtUp"
   },
   "outputs": [],
   "source": [
    "movies_df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUXaT2BvwtUq"
   },
   "source": [
    "Correlation tables are a numerical representation of the bivariate relationships in the dataset.\n",
    "\n",
    "Positive numbers indicate a positive correlation — one goes up the other goes up — and negative numbers represent an inverse correlation — one goes up the other goes down. 1.0 indicates a perfect correlation.\n",
    "\n",
    "So looking in the first row, first column we see `rank` has a perfect correlation with itself, which is obvious. On the other hand, the correlation between `votes` and `revenue_millions` is 0.6. A little more interesting.\n",
    "\n",
    "Examining bivariate relationships comes in handy when you have an outcome or dependent variable in mind and would like to see the features most correlated to the increase or decrease of the outcome. You can visually represent bivariate relationships with scatterplots (seen below in the plotting section).\n",
    "\n",
    "For a deeper look into data summarizations check out [Essential Statistics for Data Science](https://www.learndatasci.com/tutorials/data-science-statistics-using-python/).\n",
    "\n",
    "Let's now look more at manipulating DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPjS--d8wtUr"
   },
   "source": [
    "### DataFrame slicing, selecting, extracting\n",
    "\n",
    "Up until now we've focused on some basic summaries of our data. We've learned about simple column extraction using single brackets, and we imputed null values in a column using `fillna()`. Below are the other methods of slicing, selecting, and extracting you'll need to use constantly.\n",
    "\n",
    "It's important to note that, although many methods are the same, DataFrames and Series have different attributes, so you'll need be sure to know which type you are working with or else you will receive attribute errors.\n",
    "\n",
    "Let's look at working with columns first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmOc5Kk8wtU7"
   },
   "source": [
    "#### By column\n",
    "\n",
    "You already saw how to extract a column using square brackets like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hif_TbeKwtU8"
   },
   "outputs": [],
   "source": [
    "dir_col = movies_df['director']\n",
    "\n",
    "type(dir_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPqZkUMZwtU-"
   },
   "source": [
    "This will return a *Series*. To extract a column as a *DataFrame*, you need to pass a list of column names. In our case that's just a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIBRUpO9wtU_"
   },
   "outputs": [],
   "source": [
    "director_col = movies_df[['director']]\n",
    "\n",
    "type(director_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYKpIdHWwtVA"
   },
   "source": [
    "Since it's just a list, adding another column name is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuduniKjwtVB"
   },
   "outputs": [],
   "source": [
    "subset = movies_df[['director', 'rating']]\n",
    "\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYg1DQEIwtVD"
   },
   "source": [
    "Now we'll look at getting data by rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqiQvct6wtVE"
   },
   "source": [
    "#### By rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLT03UJVwtVE"
   },
   "source": [
    "For rows, we have two options:\n",
    "\n",
    "- `.loc` - **loc**ates by name\n",
    "- `.iloc`- **loc**ates by numerical **i**ndex\n",
    "\n",
    "Remember that we are still indexed by movie Title, so to use `.loc` we give it the Title of a movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysD-tgkOwtVF"
   },
   "outputs": [],
   "source": [
    "prom = movies_df.loc[\"Prometheus\"]\n",
    "\n",
    "prom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Rz_hs6-wtVG"
   },
   "source": [
    "On the other hand, with `iloc` we give it the numerical index of Prometheus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70fzidjlwtVe"
   },
   "outputs": [],
   "source": [
    "prom = movies_df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNqXhTptwtVg"
   },
   "source": [
    "`loc` and `iloc` can be thought of as similar to Python `list` slicing. To show this even further, let's select multiple rows.\n",
    "\n",
    "How would you do it with a list? In Python, just slice with brackets like `example_list[1:4]`. It's works the same way in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBoKxImBwtVk"
   },
   "outputs": [],
   "source": [
    "movie_subset = movies_df.loc['Prometheus':'Sing']\n",
    "\n",
    "movie_subset = movies_df.iloc[1:4]\n",
    "\n",
    "movie_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqqHn5F7wtVl"
   },
   "source": [
    "One important distinction between using `.loc` and `.iloc` to select multiple rows is that `.loc` includes the movie *Sing* in the result, but when using `.iloc` we're getting rows 1:4 but the movie at index 4 (*Suicide Squad*) is not included.\n",
    "\n",
    "Slicing with `.iloc` follows the same rules as slicing with lists, the object at the index at the end is not included.\n",
    "\n",
    "#### Conditional selections\n",
    "We’ve gone over how to select columns and rows, but what if we want to make a conditional selection?\n",
    "\n",
    "For example, what if we want to filter our movies DataFrame to show only films directed by Ridley Scott or films with a rating greater than or equal to 8.0?\n",
    "\n",
    "To do that, we take a column from the DataFrame and apply a Boolean condition to it. Here's an example of a Boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOeeU_okwtVm"
   },
   "outputs": [],
   "source": [
    "condition = (movies_df['director'] == \"Ridley Scott\")\n",
    "\n",
    "condition.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7aAEYlswtVt"
   },
   "source": [
    "Similar to `isnull()`, this returns a Series of True and False values: True for films directed by Ridley Scott and False for ones not directed by him.\n",
    "\n",
    "We want to filter out all movies not directed by Ridley Scott, in other words, we don’t want the False films. To return the rows where that condition is True we have to pass this operation into the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRIv3suLwtVu"
   },
   "outputs": [],
   "source": [
    "movies_df[movies_df['director'] == \"Ridley Scott\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZdW714CwtVv"
   },
   "source": [
    "You can get used to looking at these conditionals by reading it like:\n",
    "\n",
    "> Select movies_df where movies_df director equals Ridley Scott\n",
    "\n",
    "Let's look at conditional selections using numerical values by filtering the DataFrame by ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41d22jkbwtV7"
   },
   "outputs": [],
   "source": [
    "movies_df[movies_df['rating'] >= 8.6].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kb0q7drjwtV-"
   },
   "source": [
    "We can make some richer conditionals by using logical operators `|` for \"or\" and `&` for \"and\".\n",
    "\n",
    "Let's filter the the DataFrame to show only movies by Christopher Nolan OR Ridley Scott:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ip8kWuCPwtV-"
   },
   "outputs": [],
   "source": [
    "movies_df[(movies_df['director'] == 'Christopher Nolan') | (movies_df['director'] == 'Ridley Scott')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vyn2jm4wtWE"
   },
   "source": [
    "We need to make sure to group evaluations with parentheses so Python knows how to evaluate the conditional.\n",
    "\n",
    "Using the `isin()` method we could make this more concise though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PgxPjQbwtWH"
   },
   "outputs": [],
   "source": [
    "movies_df[movies_df['director'].isin(['Christopher Nolan', 'Ridley Scott'])].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqDg7Um6wtWJ"
   },
   "source": [
    "Let's say we want all movies that were released between 2005 and 2010, have a rating above 8.0, but made below the 25th percentile in revenue.\n",
    "\n",
    "Here's how we could do all of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndnotfikwtWK"
   },
   "outputs": [],
   "source": [
    "movies_df[\n",
    "    ((movies_df['year'] >= 2005) & (movies_df['year'] <= 2010))\n",
    "    & (movies_df['rating'] > 8.0)\n",
    "    & (movies_df['revenue_millions'] < movies_df['revenue_millions'].quantile(0.25))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyMTBRzfwtWL"
   },
   "source": [
    "If you recall up when we used `.describe()` the 25th percentile for revenue was about 17.4, and we can access this value directly by using the `quantile()` method with a float of 0.25.\n",
    "\n",
    "So here we have only four movies that match that criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDHHoLSBwtWM"
   },
   "source": [
    "## Applying functions\n",
    "\n",
    "It is possible to iterate over a DataFrame or Series as you would with a list, but doing so — especially on large datasets — is very slow.\n",
    "\n",
    "An efficient alternative is to `apply()` a function to the dataset. For example, we could use a function to convert movies with an 8.0 or greater to a string value of \"good\" and the rest to \"bad\" and use this transformed values to create a new column.\n",
    "\n",
    "First we would create a function that, when given a rating, determines if it's good or bad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFzvQ1zpwtWV"
   },
   "outputs": [],
   "source": [
    "def rating_function(x):\n",
    "    if x >= 8.0:\n",
    "        return \"good\"\n",
    "    else:\n",
    "        return \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAGhqkJJwtWZ"
   },
   "source": [
    "Now we want to send the entire rating column through this function, which is what `apply()` does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sH-SkrwswtWg"
   },
   "outputs": [],
   "source": [
    "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(rating_function)\n",
    "\n",
    "movies_df[[\"rating\", \"rating_category\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp2rm8cfwtWm"
   },
   "source": [
    "The `.apply()` method passes every value in the `rating` column through the `rating_function` and then returns a new Series. This Series is then assigned to a new column called `rating_category`.\n",
    "\n",
    "You can also use anonymous functions as well. This lambda function achieves the same result as `rating_function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WT1cbsEuwtWo"
   },
   "outputs": [],
   "source": [
    "movies_df[\"rating_category\"] = movies_df[\"rating\"].apply(lambda x: 'good' if x >= 8.0 else 'bad')\n",
    "\n",
    "movies_df[[\"rating\", \"rating_category\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYdN1zV2wtWp"
   },
   "source": [
    "Overall, using `apply()` will be much faster than iterating manually over rows because pandas is utilizing vectorization.\n",
    "\n",
    "> Vectorization: a style of computer programming where operations are applied to whole arrays instead of individual elements —[Wikipedia](https://en.wikipedia.org/wiki/Vectorization)\n",
    "\n",
    "A good example of high usage of `apply()` is during natural language processing (NLP) work. You'll need to apply all sorts of text cleaning functions to strings to prepare for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLOX_p4LwtXJ"
   },
   "source": [
    "## Wrapping up\n",
    "\n",
    "Exploring, cleaning, transforming, and visualization data with pandas in Python is an essential skill in data science.\n",
    "\n",
    "To keep improving, view the [extensive tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html) offered by the official pandas docs, and solve the next practice exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pandas Practice\n",
    "\n",
    "Load the `titanic.csv` dataset into a dataframe. Use `PassengerId` as the `index_col`. Take a look at the top rows."
   ],
   "metadata": {
    "id": "mIWhPuBaHhWz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "df =\n",
    "df.head()\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "NMiBuopI8qIb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now inspect its general properties using `info()`. Are there any missing values?"
   ],
   "metadata": {
    "id": "0af-e2hqIh2p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.info()"
   ],
   "metadata": {
    "id": "boRlqXeyGkjt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the total number of missing values for each attribute."
   ],
   "metadata": {
    "id": "oC8q7UlJIqCy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df.isnull().sum()"
   ],
   "metadata": {
    "id": "XyURsf6mJwCX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have three columns with missing values, however the total numbers are quite different. Start cleaning the data by dropping the two rows with missing `Embarked` attributes (check the documentation on the [dropna() function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html?highlight=dropna#pandas.DataFrame.dropna))"
   ],
   "metadata": {
    "id": "A8q-CtngKyEO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Drop rows with missing Embarked values\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "w7jY6hhLLWjp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are missing almost all `Cabin` values, drop the column from your dataframe (check the [drop()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) function)."
   ],
   "metadata": {
    "id": "m3sEe4mpLi3p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Drop Cabin column\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "_T625XgJLr0u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, identify categorical atributes and encode them using one-hot encoding (check the documentation for the [get_dummies](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function, pay special attention to the `drop_first` argument).\n",
    "\n",
    "\n",
    "---\n",
    "Note that this step can also be accomplished using [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html), but for this excercise use `pandas` (the issue of which to use is a bit [contentious](https://towardsdatascience.com/one-hot-encoding-scikit-vs-pandas-2133775567b8))"
   ],
   "metadata": {
    "id": "UrnAsYiFOfVc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Encode categorical attributes\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "k0WdCFeZOim1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, les impute the missing `Age` values by filling them with the mean grouped by `Sex`. Acheving this will require a couple of steps. First group the data by the `Sex` column and apply a `transform('mean')` to the `Age` column (check the documentation for the\n",
    "[DataFrameGroupBy.transform()](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.transform.html?highlight=groupby%20transform#pandas.core.groupby.DataFrameGroupBy.transform) function).\n",
    "Round the resulting Series and save it as `mean_age`.This should result in a Series formed by mean ages corresponding to the row's sex.\n",
    "\n",
    "```\n",
    "PassengerId\n",
    "1      31.0\n",
    "2      28.0\n",
    "3      28.0\n",
    "4      28.0\n",
    "5      31.0\n",
    "       ...\n",
    "887    31.0\n",
    "888    28.0\n",
    "889    28.0\n",
    "890    31.0\n",
    "891    31.0\n",
    "Name: Age, Length: 889, dtype: float64  \n",
    "```"
   ],
   "metadata": {
    "id": "OZvsbEqdLvFd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "mean_age =\n",
    "print(mean_age)\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "9haZTvw3NNPW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second, use the `mean_age` Series to fill in the missing values of the dataset (check the [fillna()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html?highlight=fillna#pandas.DataFrame.fillna) function)."
   ],
   "metadata": {
    "id": "v6HKKQJPZRgs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "df['Age'] =\n",
    "df.isnull().sum()\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "TsSzfqBcaBIV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, create a new column named `Family` consisting of the total number of relatives travelling with the individual (you'll need to add the `SibSp` and `Parch` columns)."
   ],
   "metadata": {
    "id": "cOb2qA4oa98c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "df.head()\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "TR7QNkSWbaFn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, split the dataframe into two separate dataframes: one for training attributes (predictors, or x) and another for the target value (class, or y)."
   ],
   "metadata": {
    "id": "91fRnNhub5a4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "df_x =\n",
    "df_x.head()\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "PhsRFqw9gUUM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "df_y =\n",
    "df_y.head()\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "VglG9Y6LgXHE"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
